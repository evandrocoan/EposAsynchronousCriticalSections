!::Predictable Synchronization Algorithms %%% for Asynchronous Critical Sections::
::Evandro Sperfeld Coan <evandrocoan@hotmail.com>::
::Eduardo Demeneck Onghero <do.demeneck@gmail.com>::

{maketoc}

!!Motivação  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
A abordagem tradicional para a coordenação do acesso à recursos compartilhados em ambientes multi thread envolve a utilização de algoritmos bloqueantes de sincronização. Esses algoritmos forçam threads a bloquearem nos casos de disputas no acesso à recursos compartilhados.

Uma outra abordagem possível envolve o uso de algoritmos baseados em delegação e sincronização não bloqueante para a coordenação de threads. Com esse tipo de algoritmo, threads delegam a execução de operações envolvendo acessos a recursos compartilhados à outras threads. A sincronização não bloqueante é utilizada para garantir certas propriedades às operações dos algoritmos baseados em delegação, de modo a evitar problemas decorrentes do bloqueio de threads, como variações de latência e inversões de prioridade, altamente relevantes no contexto de sistemas embarcados multi core, que, por interagirem diretamente com objetos físicos, possuem necessidades diferenciadas de latência e throughput.

!!Objetivos  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
O objetivo principal deste trabalho é a implementação de um algoritmo de sincronização com suporte à seções críticas assíncronas.

!!!Objetos específicos  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
*Implementação de um algoritmo de sincronização baseado em delegação para seções críticas assíncronas, utilizando o conceito seções guardadas.
*Implementação de uma nova versão do algoritmo com suporte para seções críticas síncronas.
*Realização de testes para avaliar a corretude dos algoritmos implementados.

!!Metodologia  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Será realizada uma pesquisa na literatura com foco nos conceitos básicos que fundamentam algoritmos de sincronização para seções críticas assíncronas, de modo a compreender detalhes de funcionamento desse tipo de algoritmo e então decidir sobre a melhor maneira de implementá-los no EPOS.

Após entendidos claramente os conceitos, o desenvolvimento seguirá com a metodologia programar e testar (Cowboy Coding), que consiste em escrever um pedaço de código e testar sua execução, verificando se o resultado consiste com o esperado. Os programas de teste serão programas simples, que permitam verificar se o sistema está respondendo corretamente às expectativas.

!!Cronograma  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
||
Tarefa       | 01/10 | 08/10 | 22/10 | 05/11 | 19/11 | 26/11 | 21-28/11
T0           | D0    |       |       |       |       |       |
T1           |       | D1    |       |       |       |       |
T2           |       |       | D2    |       |       |       |
T3           |       |       |       | D3    |       |       |
T4           |       |       |       |       | D4    |       |
T5           |       |       |       |       |       | D5    |
Apresentação |       |       |       |       |       |       | Ap
||

!!Tarefas e Entregáveis  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]

!!!T0 - Planejamento  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
* Escrever o plano de projeto detalhado, contendo uma fundamentação teórica robusta, apresentando os conceitos chave relacionados à algoritmos de sincronização para seções críticas assíncronas, assim como exemplos de algoritmos desse tipo.  Além disso, o plano deve conter o detalhamento do projeto, explicando o que será feito durante o semestre e quais entregáveis serão produzidos.
* Escrever programas de teste que provem a viabilidade do projeto, demonstrando que as operações CAS e FAS, necessárias para a implementação do algoritmo, são suportadas pelo EPOS.
__Entregável: D0__
# Plano de projeto
## Descrição da metodologia adotada.
## Lista de tarefas/entregáveis para cada entrega com seu cronograma.
## Fundamentação teórica.

!!!T1 - Revisão do Planejamento  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
* Realizar as correções no plano de projeto com base no feedback obtido no D0.
__Entregável: D1__
# Plano de projeto revisado.

!!!T2 - Implementação I  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
* Implementação de um novo componente de sincronização no EPOS. Esse componente será responsável por prover mecanismos para a execução não bloqueante de seções críticas assíncronas. Uma limitação dessa primeira versão do componente está relacionada aos tipos de seções críticas suportadas, que no caso serão apenas seções críticas assíncronas. Isso porque, o suporte a seções críticas síncronas exige a utilização de mecanismos para a sincronização unilateral de threads, que serão o tema principal da tarefa T3.
* Definição de uma convenção de programação, que poderá ser utilizada por threads do EPOS para solicitar a execução assíncronas de seções críticas, e que juntamente com o componente Guard, compõe um sistema de execução que possibilita a execução de seções críticas assíncronas de maneira não bloqueante.
__Entregável: D2__
# Novo componente de sincronização do EPOS, o Guard.
# Uma convenção de programação para a submissão de requisições de execução de seções críticas.

!!!T3 - Implementação II  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
* Estender o componente de sincronização apresentado em T2 para suportar também seções críticas síncronas. Para isso, serão utilizados ''futures'', cuja implementação será baseada em uma descrição apresentada em "Guarded sections: Structuring aid for wait-free synchronisation". G. Drescher and W. Schröder-Preikschat (2015) {DIV(type="span")}[{DIV}[#Refer_ncias_|2]].
* Modificar a convenção de programação para integrar os mecanismos de ''futures'', de modo a intermediar a comunicação entre threads e seções críticas bloqueantes.
__Entregável: D3__
# Componente Guarda com suporte a seções críticas síncronas.
# Novo mecanismo de ''futures''.

!!!T4 - Implementação III  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
* Aprimorar o modo como as seções críticas são definidas e permitir que sejam definidas seções críticas a partir de funções com um número arbitrário de parâmetros, assim como ocorre com threads.
* Substituir os mecanismos de sincronização das aplicações de teste pela nova implementação com uso de guarda.
__Entregável: D4__
# As aplicações "synchronizer_test.cc", "scheduler_cpu_affinity_test.cc" e "semaphore_test.cc" utilizando o algoritmo de guarda em vez de mutexes e semáforos.

!!!T5 - Validação  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
* Realização de testes em nível acadêmico, com o objetivo de garantir, na medida do possível, a corretude dos componentes implementados até aqui.
* Desenvolvimento de um relatório final, onde será apresentado o que foi feito durante o projeto e quais foram seus resultados.
__Entregável: D5__
# Relatório final, que é uma versão do relatório inicial com as correções de planejamento adicionadas ao longo da execução do projeto.
# O código dos novos testes realizados, junto com o resultado dos testes

!!!Tarefas Opcionais  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
# Uma segunda versão da implementação de Futures, com suporte completo requisições bloqueantes e não bloqueantes junto com um serviço de execução que trata de criar novas threads na medida que o sistema necessita executar códigos com dependência de dados. Ver a seção [#Futures_|Futures]. Tal serviço será reconfigurável, limitando o número máximo de threads que podem existir simultaneamente. Por exemplo, um sistema embarcado pode não possuir memória suficiente para que muitas thread sejam criadas, então esse serviço de threads limitaria o número máximo de threads que podem ser criadas ao mesmo tempo, e caso esse limite seja ultrapassado, novas requisições aguardam em uma fila, i.e., bloqueiam até que recursos estejam disponíveis. %%% Tal serviço também, pode ser configurado para iniciar com um número definido de threads que ficam aguardando um bloco de dependência ser requerido, e na medida que surge o diminui a demanda, vai criando e destruindo threads para economizar memória. Por preferência de performance e de não remover recursos do sistema, somente fazer tal balanceamento de carga quando uma idle thread entrar em execução. Assim, não se tira recursos de nenhuma outra thread em execução.
# A implementação da variação do algoritmo do __Guarda__ para arquiteturas NUMA {[https://en.wikipedia.org/wiki/Non-uniform_memory_access|2]}, o algoritmo do __Ator__. Ver a seção [#Atores_|Atores].

!!Fundamentação Teórica  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Antigamente quando não existia poder computacional, não existia o conceito atual de threads por que não havia memória suficiente. Assim toda a programação de múltiplas tarefas deveria ser feita pelo programador manualmente, para poder-se extrair o máximo da máquina. Agora, hoje em dia com processadores com milhares de núcleos, volta a ter que se programar a máquina mais diretamente para poder garantir 100% (cem por cento) de throughput de todos os núcleos dos processadores.

O trabalho de referência {DIV(type="span")}[{DIV}[#Refer_ncias_|1]] propõe dois algoritmos de baixo nível, um para arquiteturas UMA {[https://en.wikipedia.org/wiki/Uniform_memory_access|1]}, outro para arquiteturas NUMA {[https://en.wikipedia.org/wiki/Non-uniform_memory_access|2]}, que permitem que o programador programe a iteração com a máquina, de forma que ele diga o que a máquina deve fazer, enquanto aguarda que suas seções críticas sejam executadas assincronamente. Regiões críticas assíncronas significam que threads podem requisitar a execução de arbitrárias regiões críticas, sem ter que bloquear a sua execução. Somente a região crítica terá sua execução adiada, devido a necessidade da exclusão mútua. Enquanto a região crítica aguarda sua execução em uma fila, a thread principal pode continuar sua execução. Em contraste, as tradicionais regiões críticas síncronas, exigem que a thread bloqueie a sua execução até que a região crítica seja executada.

Entretanto, note que o programador da aplicação deve tratar de isolar explicitamente em seu código quais são as suas regiões críticas com normalmente faz, e chamar/enviar essas regiões críticas para a fila da exclusão mútua dos algoritmo utilizado. A diferença é que elas não serão mais bloqueantes, assim o programador deve prever o que irá acontecer quando ele chegar em um ponto de execução de seu código, que é dependente dos resultados da execução da região crítica. Assim, no pior dos casos, a execução da thread terá que bloquear a execução, enquanto aguarda a chegada das novas informações provenientes da região crítica.

!!!Sincronização não bloqueante  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Em ambientes multi thread, técnicas de sincronização precisam ser empregadas para coordenar o acesso à recursos compartilhados e evitar a ocorrência de problemas, como por exemplo, condições de corrida, que podem levar o sistema a se comportar de maneira indefinida ou errônea. No contexto da sincronização de threads, as partes do programa onde recursos compartilhados são acessados por diferentes threads são chamadas de seções críticas.

Técnicas tradicionais de sincronização utilizam mecanismos bloqueantes para coordenar a execução de seções críticas, impedindo que diferentes seções críticas executem simultaneamente e garantindo a propriedade de exclusão mútua. Nesses casos, quando uma thread tenta executar uma seção crítica, e os recursos compartilhados relacionados à essa seção crítica estão ocupados, a thread é bloqueada até que esses recursos sejam liberados. Esse comportamento simplifica a implementação de sistema concorrentes, mas pode levar a problemas de inversão de prioridade, tempos de espera indefinidos, convoying e até deadlocks.

Uma estratégia diferente para lidar com a sincronização de threads envolve a utilização de mecanismos e técnicas de sincronização não bloqueantes. Diferente do que ocorre com as técnicas tradicionais, no caso da sincronização não bloqueante, recursos compartilhados podem ser acessados concorrentemente por múltiplas threads sem a necessidade de bloqueio. Para isso, algoritmos não bloqueantes são cuidadosamente construídos com base em primitivas de sincronização atômicas do tipo ''read-write-modify'', geralmente implementadas em hardware, e podem ser utilizados para aumentar o nível de paralelismo do sistema.

Um nicho onde a sincronização não bloqueante se apresenta como uma alternativa viável é no mundo das aplicações para sistemas embarcados multicore. Como essas aplicações geralmente lidam diretamente com entidades físicas, elas se beneficiam de algumas das vantagens desse tipo de algoritmo, como suas garantias de progresso e a preditibilidade de latência de suas operações.

!!!Garantias de Progresso  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Algoritmos de sincronização não bloqueantes podem ser classificados de acordo com as garantias de progresso de suas operações. Quando todas as operações de um determinado algoritmo oferecem garantias de progresso de um determinado nível, ou de níveis superiores, pode-se dizer esse é o nível de garantias de progresso oferecido pelo algoritmo como um todo. As garantias de progresso de operações de sincronização não bloqueantes são classificadas em três níveis:

*Obstruction-free: É a garantia de progresso de nível mais baixo. Uma operação é considerada obstruction-free se uma thread, executando de maneira isolada, sem sofrer interferências, i.e. competir com outras threads por recursos {[https://www.cs.rochester.edu/~scott/papers/2006_PPoPP_synch_queues.pdf|3] section 2.1}, consegue completar sua execução em um número finito de passos.

*Lock-free: Um operação é lock-free, se quando invocada por múltiplas threads, garante que pelo menos uma delas threads irá terminar em um número finito de passos. Dessa forma, operações lock-free nunca sofrem com problemas de deadlock e livelock {[https://softwareengineering.stackexchange.com/questions/141271/if-i-use-locks-can-my-algorithm-still-be-lock-free|4]}, pois pelo menos uma das threads executando a operação irá progredir. Implementações de soluções lock-free geralmente envolvem a utilização de laços, que geralmente envolvem a primitiva atômica CAS, onde threads repetem certos passos um número arbitrário de vezes, que depende da comportamento da operação, até conseguir prosseguir. Certos autores referem-se a algoritmos lock-free e algoritmos não bloqueantes de maneira análoga, o que pode gerar certa confusão, já que nem todo algoritmo não bloqueante é lock-free {[https://www.justsoftwaresolutions.co.uk/threading/non_blocking_lock_free_and_wait_free.html|5]}, pois podem também ser categorizados como wait-free ou obstruction-free.

* Wait-free: Uma operação é dita wait-free, se, quando invocada por múltiplas threads, garante que todas irão terminar em um número finito de passos. Essa propriedade é especialmente importante para sistemas com grande escala de concorrência, pois independentemente do número de threads executando operações wait-free concorrentemente, nenhuma jamais precisará esperar ou bloquear {DIV(type="span")}[{DIV}[#Refer_ncias_|13]], e como o progresso é garantido para todas as threads, nenhuma sofrerá com problemas de ''starvation''.

!!!Primitivas de Sincronização  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
A criação de algoritmos de sincronização não bloqueantes pode envolver a utilização de primitivas de sincronização atômicas do tipo ''read-write-modify''. Essas primitivas geralmente leem uma posição de memória e, simultaneamente, escrevem um novo valor em seu lugar. Exemplos de operações atômicas desse tipo são: compare-and-swap (CAS), fetch-and-operation (FAθ), load-link/store-conditional (LL/SC) e test-and-set (TAS). A seguir serão detalhadas duas dessas operações, CAS e FAS (fetch-and-store), uma especialização da operação FAθ. Ambas são utilizadas na implementação de um algoritmo de sincronização não bloqueante que será apresentado mais adiante.

A operação FAS realiza, de maneira atômica, uma leitura e uma escrita em um endereço de memória específico, e pode ser entendida como uma versão atômica do pseudocódigo, apresentado a seguir:
  {CODE(colors="c")}
int fas(address * location, int replacement) {
        int old = *location;
        *location = replacement;
        return old;
}
{CODE}
A operação CAS, cuja implementação lógica em pseudocódigo pode ser vista abaixo, possui um funcionamento similar ao da operação FAS, no entanto, a escrita no endereço de memória sendo acessado ocorre de forma condicional, acontecendo apenas caso o resultado da comparação entre o valor atual desse endereço e o valor de uma determinada variável seja positivo. Na prática, a escrita só acontece caso o valor do endereço sendo acessado já é conhecido por quem está executando o CAS.

{CODE(colors="c")}
int cas(address location, int compare, int replacement) {
    int old = *location;
    if(*location == compare) {
        *location = replacement;
    }
    return old;
}
{CODE}
Um detalhe importante sobre essas operações é o fato de processadores geralmente não suportarem todos os tipos de primitivas ''read-write-modify'' em hardware. A arquitetura RISC-V, por exemplo, implementa apenas as operações FAθ e LL/SC, enquanto a arquitetura x86 não implementa a operação LL/SC, mas implementa a operação CAS.

!!!Sincronização Baseada em Delegação  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Com algoritmos de sincronização baseados em delegação, threads podem delegar a execução de suas seções críticas à outras threads. Com isso, as seções críticas são desacopladas das threads que requisitam sua execução, que agora podem decidir se continuam trabalhando, sem bloquear, após delegar a execução de uma seção crítica, ou se preferem bloquear e continuar apenas após a execução da seção crítica terminar. Além disso, deve haver algum protocolo ou sistema de execução cujos mecanismos garantam que as seções críticas eventualmente serão executadas e que a propriedade da exclusão mútua será mantida.

Técnicas de sincronização baseadas em delegação podem definir uma única thread, geralmente presa a um core específico, como responsável pela execução das seções críticas do programa. Um dos benefícios dessa abordagem é o melhor aproveitamento da localidade de dados das caches do sistema, já que agora, todos os acessos aos dados das seções críticas são feitos pela mesma thread. Uma outra possível abordagem, permite que a responsabilidade de execução de seções críticas seja transferida entre threads de acordo com as necessidades do sistema. Nesse caso, diminui-se o aproveitamento da localidade das caches, mas remove-se a necessidade de se manter uma thread, ou até mesmo um core do processador, restrita a execução de seções críticas.

Em situações ideais, onde threads não precisam do resultado de suas seções críticas, ou seja, não existem dependências unilaterais de dados entre a seção crítica e a thread requisitando sua execução, o bloqueio dessas threads se torna opcional. Seções críticas que exibem essas características são chamadas de seções críticas assíncronas. Algoritmos de sincronização baseados em delegação para seções críticas assíncronas permitem que threads deleguem a execução de seções críticas utilizando mecanismos do tipo fire-and-forget, podendo continuar com sua própria execução e esquecer sobre a seção crítica cuja execução foi requisitada.

No entanto, isso nem sempre é possível e, em alguns casos, a sincronização baseada em delegação traz novos desafios. Caso existam dependências de dados entre uma seção crítica e a thread que requisitou sua execução, essa seção crítica é caracterizada como uma seção crítica síncrona. Com esse tipo de seção crítica, surge a necessidade de bloqueio da thread requerente, o que pode ocorrer no momento em que é feita a requisição de execução, ou apenas quando os dados que podem ainda não ter sido calculados, são acessados. No primeiro caso, podem ser utilizados mecanismos de bloqueio simples, como semáforos ou mutexes, enquanto no segundo, exige-se a utilização de mecanismos mais complexos, como ''futures'' e ''observables''.

!!Guardas  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Todos os conceitos apresentados até aqui convergem em um algoritmo de sincronização, baseado em delegação, para seções críticas assíncronas, que dá origem a uma nova entidade de sincronização: a Guarda. Guardas possuem duas operações básicas, uma para a entrada e uma para a saída de seções críticas, vouch e clear, respectivamente, além de uma convenção de programação, apresentada na Listagem 1, que deve ser utilizada pelas threads do programa para requisitar a execução assíncrona de seções críticas.

::__Listagem 1:__ Protocolo para a requisição de execução de seções críticas.::
{CODE(colors="c")}
Guard guard = ...; // Shared
...
Job * job = ...
Job * cur;
if (NULL != (cur = guard.vouch(job))) do {
    run(cur);
} while (NULL != (cur = guard.clear()));
{CODE}

O algoritmo da guarda também introduz o conceito de uma entidade chamada ''sequencer'', que será responsável pela execução das seções críticas do programa. Toda thread, após emitir uma requisição de execução de seção crítica, pode ter que assumir o papel de ''sequencer''. Ao assumir esse papel, a thread deve trabalhar na execução de seções críticas até que não hajam mais requisições pendentes a serem executadas. A convenção de programação definida para submissão de requisições, junto com o modo de funcionamento das operações vouch e clear, impede que mais de um ''sequencer'' esteja ativo ao mesmo tempo, fazendo com que a execução das seções críticas ocorra de forma sequencial, garantindo a propriedade da exclusão mútua.

::{img fileId="931"}::
::__Figura 1:__ Diagrama demonstrando o funcionamento do protocolo de submissão de seçẽos críticas. Fonte: Própria::

Em termos gerais, o funcionamento do protocolo de submissão de requisições de execução de seções críticas à guarda, que pode ser observado no diagrama da Figura 1, se dá da seguinte maneira: seguindo a convenção de programação estipulada, threads devem realizar um vouch para submeter requisições à guarda. Caso já exista um ''sequencer'' ativo, vouch retornará ''NULL'', sinalizando para a thread requerente que ela pode continuar com sua execução. No entanto, caso nenhuma thread esteja atuando como ''sequencer'', vouch retornará um job, sinalizando para a thread que ela deve atuar como ''sequencer'', começando pela execução do job retornado. Após terminar a execução desse job, a thread ''sequencer'' invoca a operação clear repetidamente, e enquanto houverem seções críticas à serem executadas, clear retornará novos jobs, que também serão executados pelo ''sequencer''. Quando não houverem mais seções críticas à serem executadas, clear retornará ''NULL'', sinalizando à thread que ela deve abandonar o papel de ''sequencer'' e continuar com sua execução normal.

::{img fileId="933"}::
::__Figura 2:__ Atributos da guarda e de seus elementos. Fonte: Própria::

A partir da descrição geral do funcionamento do algoritmo, percebe-se que guardas comportam-se como filas, onde jobs representando seções críticas são armazenados. A Figura 2 representa a estrutura geral de uma guarda e de seus elementos, que correspondem a elementos de fila simples, com apenas um nível de encadeamento. Além de ser do tipo FIFO, como apenas o ''sequencer'' pode remover elementos, a fila representada pela guarda pode ser considerada uma fila multiple-producer-single-consumer (MPSC), pois, enquanto várias threads podem, simultaneamente, inserir elementos na guarda, apenas o ''sequencer'' pode removê-los. Essa característica diminui muito a complexidade de implementação de operações wait-free para a inserção e remoção de elementos, como poderá ser notado quando o código das operações vouch e clear for apresentado.

Uma implementação para vouch é apresentada na Listagem 2. Essa implementação utiliza as primitivas atômicas CAS e FAS, sinalizadas por V1 e V2 no código, para coordenar acessos a elementos da guarda compartilhados por múltiplas threads, de maneira a evitar a ocorrência de condições de corrida. Mais especificamente, FAS é utilizada, em V1, para coordenar a inserção de novos elementos na guarda por invocações distintas de vouch. Enquanto CAS é utilizada, em V2, para lidar com o caso especial onde a guarda possui apenas um elemento, que pode ser acessado simultaneamente por vouch e clear, e, portanto, deve ser acessado de maneira coordenada.

::__Listagem 2:__ Implementação da operação vouch.::
{CODE(colors="c")}
Element * vouch (Element * item) {
    item->next = NULL ;
    Element * last = FAS(_tail, item ); // V1
    if (last && CAS (last->_next, NULL, item )) // V2
        return NULL ;
    _head = item ; // V3
    return item ;
}
{CODE}

A operação clear é responsável por remover elementos da guarda. Implementações wait-free de operações de remoção de elementos de estruturas de dados tendem a ser extremamentes complexas, no entanto, como destacado previamente, o comportamento MPSC da guarda acaba simplificando muito o código de clear, apresentado na Listagem 3. Assim como no caso de vouch, as operações FAS e CAS são utilizadas para garantir que invocações simultâneas de clear por múltiplas threads nunca resultem em condições de corrida. No trecho de código sinalizado por C1 na Listagem 3, a operação FAS é utilizada para coordenar a definição de qual elemento deve ser a próxima cabeça da guarda, cujo valor é armazenado na variável next, além disso, FAS também marca, simultaneamente, next->_next com o valor mágico DONE, utilizado para controlar a interação entre execuções simultâneas de  vouch e clear, onde pode haver a necessidade de uma transição de ''sequencer''. Caso a variável next tenha recebido o valor ''NULL'', tem-se que a lista possui menos de um elemento, e a semântica de clear indica que não apenas a cabeça da lista deve ser modificada mas também a sua cauda, nesse caso, ambas modificações afetam o mesmo elemento e precisam ser coordenadas, por isso são implementadas por operações CAS.

As implementações de vouch e clear apresentadas são wait-free, pois permitem a invocação dessas operações por múltiplas threads com garantias de progresso para cada uma delas. Dessa forma, caso já exista um ''sequencer'' ativo, o progresso wait-free de todas as threads que submeterem requisições à guarda é garantido, pois esse processo envolve apenas uma invocação à vouch, que também é wait-free. No entanto, as garantias de progresso se tornam mais complexas para o caso de threads que precisam assumir o papel de ''sequencer''. Em sistemas onde threads submetem seções críticas à guarda com muita frequência, uma thread que assume o papel de ''sequencer'' pode ter que executar seções críticas indefinidamente, o que a impedirá de progredir com o resto do seu código. Além disso, seções críticas maliciosas, que executam indefinidamente, também podem impedir o progresso do ''sequencer''. No entanto, esse tipo de seção crítica nunca deve existir, e sua presença pode ser considerada um erro de programação.

::__Listagem 3:__ Implementação da operação clear.::
{CODE(colors="c")}
Element * clear() {
    Element * item = _head;
    Element * next = FAS(item->_next, DONE); // C1
    if (!next)
        CAS(_tail , item, NULL); // C2
    CAS(_head, item, next);  // C3
    return next ;
} 
{CODE}

Até agora, o algoritmo tratou apenas de seções críticas assíncronas, onde o código das threads não depende de dados calculados nas seções críticas, mas esse nem sempre é o caso. Dessa forma, o algoritmo da guarda pode ser estendido para lidar também com seções críticas síncronas. Para isso, são adicionados mecanismos como ''futures'' {DIV(type="span")}[{DIV}[#Refer_ncias_|14]], que permitem a comunicação de dados entre threads e suas seções críticas, de modo a satisfazer dependências unilaterais de dados.

Além dos problemas já citados, várias considerações podem ser feitas em relação à threads de prioridades diferentes compartilhando uma mesma guarda. Nesses casos, inversões de prioridade podem acontecer nos casos onde threads de alta prioridade se tornam ''sequencer'' e são forçadas a executar seções críticas de threads de baixa prioridade, ou em casos onde seções críticas relacionadas à threads de alta prioridade são executadas por threads de baixa prioridade. Para solucionar esses problemas, podem ser desenvolvidas variações do algoritmo, onde o papel de ''sequencer'' pode ser renegociado para que as definições de prioridade do sistema sejam respeitadas.

!!!Atores  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Existe uma variação do algoritmo da guarda, chamado de algoritmo dos atores, onde a execução das seções críticas é realizada por uma thread servidora dedicada. Em aspectos funcionais, os dois algoritmos são muito parecidos. Ambos possuem duas operações principais, uma para a inserção e uma para a remoção de seções críticas de filas, e uma convenção de programação que deve ser utilizada pelas threads do programa para delegar a execução de suas seções críticas.

Algoritmos de sincronização baseados em delegação onde uma única thread dedicada é responsável pela execução de seções críticas aproveitam ao máximo a localidade no acesso à dados compartilhados, mas pagam um preço por restringirem uma thread, ou até mesmo um core do processador, apenas à execução de seções críticas. Devido a essas característica, o algoritmo dos atores tende a se comportar melhor em sistemas com muitos núcleos de processamento (many-core).

Outro detalhe a ser considerado com esse tipo de abordagem é o overhead decorrente da necessidade da thread dedicada ser posta para dormir quando não existem seções críticas à serem executadas.

!!!Comparação entre o uso do guarda e semáforo  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Uma versão equivalente ao uso de guardas, pode ser implementada utilizando várias threads e um semáforo inicializado em 1 para controle da entrada da seção crítica, i.e., exclusão mútua. Na implementação apresentada para o algoritmo de guardas, utiliza-se um ponteiro de função para passar para o ''sequencer'', qual será a seção crítica a ser executada. Enquanto, na versão com threads, teria-se que criar uma nova thread, além da thread atual, passando um ponteiro de função para a seção crítica. E então essa nova thread irá testar o valor do semáforo antes de executar a seção crítica.

A seguir, vê-se resumidamente o fluxo de execução do algoritmo do guarda, já explicado anteriormente. Nele, a ''Thread A'' primeiro empilha um ponteiro de função __job__ na fila de tarefas da variável global __my_guard__ e então verifica se a ''Thread A'', deve ou não se tornar o ''sequencer''. Como a ''Thread A'' é a primeira thread a empilhar uma região crítica, __guard_vouch()__ retorna um valor não nulo, indicando a próxima tarefa a ser executada. Assim, a ''Thread A'' torna-se o ''sequencer''. E logo em seguida, chama o método __do_things()__ que executa a função que contém a região crítica representado por __next_job__. No final da execução, a ''Thread A'' chamará __guard_clear()__, que neste ambiente de exemplo retornará ''NULL'', pois não há outras threads/seções críticas.
::{img fileId="943"}::
::__Fonte:__ Própria::

Agora, vê-se um equivalente do mesmo exemplo do guarda, mas utilizando um semáforo global inicializado em 1 para todas as threads, pois para garantir exclusão mútua somente pode existir uma thread executando a região crítica. Primeiro, cria-se uma thread com new, passando como parâmetro um __semáforo__ compartilhado e um ponteiro de função __job__. É importante notar que não se faz __join()__ nesta thread, pois se está tratando de uma seção crítica assíncrona, e precisa-se também de uma implementação especial de thread que antes de chamar o ponteiro de função de __job__, dê um ''lock()'' no semáforo, e depois de completar a função, delete a si própria. Comparando este novo código com o algoritmo do guarda, vê-se que ele ficou muito mais simples, entretanto, perde-se muito na eficiência pois para executar uma seção crítica assincronamente, tem-se que criar exclusivamente uma nova thread para cada uma das threads já existentes, tendo a criação do dobro de threads no sistema em um momento de alta competição pela seção crítica.
::{img fileId="946"}::
::__Fonte:__ Própria::

O leitor mais atento pode perceber que, abordou-se somente uma implementação sem dependência de dados com a seção crítica e, que a versão do algoritmo do guarda apresentado neste exemplo também não executou assincronamente. Somente a segunda versão, que utiliza um semáforo fez uma execução assíncrona da seção crítica. Este é um dos problemas do algoritmo do guarda. A thread que for eleita como ''sequencer'', será impedida de executar sua seção crítica assincronamente, pois ela própria tem que executar a sua seção crítica mais as seções críticas das outras threads que empilharem seus __jobs__. Com isso, pode-se ter como já falado anteriormente, o problema de ''starvation'' do ''sequencer'', que pode infinitamente continuar recebendo novos __jobs__ para executar. Entretanto, ter-se somente uma thread exclusivamente executando todas as seções críticas, traz-se a vantagem de localidade da cache caso o ''sequencer'' execute sempre em um mesmo núcleo do processador.

Outro problema que tanto a versão com semáforo quanto a versão com guardas apresentam, é quando a seção crítica é uma função recursiva. No caso do semáforo, o sistema entrará em deadlock durante a primeira recursão, pois devido a exclusão mútua que o algoritmos possuem, somente existe uma função executando a seção crítica ao mesmo tempo, e o deadlock será imediato ao início da chamada recursiva. A diferença é que o deadlock, não impede o progresso das regiões não críticas do sistema, por que por exemplo, na segunda vez que for chamado __guard_vouch()__, ele irá empilhar um ponteiro de função __job__ que irá esperar para sempre, mas o ''sequencer'' continuará executando o resto do programa que vem depois da seção crítica, pois este é o comportamento do guarda quando já existem __jobs__ na seção crítica.

Este foi um exemplo de problema tanto do algoritmo do guarda quando com semáforos, onde uma região crítica pode impedir que seções críticas sejam executadas, mas permitir que as regiões não críticas continuem executando. Note que isso pode causar falta de memória no sistema pois, toda que uma seção crítica tentar ser executada, mais um ponteiro de função (ou thread no caso do semáforo) será empilhado na fila de execução do ''sequencer'', que ficará eternamente como ''sequencer'' sem executar mais nenhuma linha de código de seções críticas.

Outro problema muito similar do algoritmo do guarda, é que quando a seção crítica é explicitamente  bloqueada por algum motivo, seja uma operação de IO ou seja por que ela possuía um semáforo que bloqueou. Bloqueios dentro da seção crítica no algoritmo do guarda, causarão diretamente a degradação/atraso da execução de todas as seções críticas do sistema, pois é somente a thread do ''sequencer'' que é habilitado a executar as seções críticas. Note também que na versão assíncrona implementada com semáforo, terá-se a possibilidade de travar todo o sistema caso aconteça o mesmo problema, pois mesmo que cada thread execute isoladamente, atrasos na execução de uma seção crítica, atrasam a liberação do semáforo, o que causa o atraso no sistema como um todo.

!!!Futures  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Seguindo as referências bibliográficas, encontrou-se que ''futures'' foram descritas pela primeira vez em 1977 no artigo "The incremental garbage collection of processes”. ''Futures'' facilitam a resolução do problema de bloqueio por dependência de dados. No diagrama de sequência a seguir, como implementar o bloqueio no ''bloco 4'' e sinalizar para o ''bloco 4'' que o ''bloco 2'' já terminou, enquanto o ''bloco 3'' executa?
::{img fileId="969"}::
::__Fonte:__ Própria::

O ''bloco 4'' somente pode ser executado quando a sessão crítica no ''bloco 2'' já tiver sido executada. Uma vez que o fluxo de execução chega nesse ponto, a ''Thread 1'' deve bloquear caso o ''bloco 2'' ainda não tenha sido executado pelo ''sequencer''. Isso não seria um problema para o trabalho por que quando existe dependência de dados, o código que é dependente fica encapsulado em uma __closure__ {DIV(type="span")}[{DIV}[#Refer_ncias_|12]] que é chamada quando o resultado do __bloco 2__ fica pronto. A seguir ve-se o fluxo de execução desse caso com o uso de ''futures'':
::{img fileId="950"}::
::__Fonte:__ Própria::

Nessa nova versão com ''futures'' vê-se um novo problema não abordado pelo artigo de referência {DIV(type="span")}[{DIV}[#Refer_ncias_|1]]. Nesse fluxo de execução, quem deve executar o ''bloco 4''? A ''Thread A'' não pode mais executar este bloco porque ele foi condicionado a ser executado depois do ''bloco 2'', que é executado assíncronamente pelo ''sequencer''. Entretanto, não pode-se deixar que o ''sequencer'' execute o ''bloco 4'' por que o ''sequencer'' somente é encarregado de executar as seções críticas, e permitir que ele execute outros blocos não críticos, irá atrasar toda pipeline de execução de seções críticas.

Isso trás uma alternativa implementação de ''futures'' que o artigo {DIV(type="span")}[{DIV}[#Refer_ncias_|1]] sugere, onde cada ''future'' possui um semáforo acoplado, e a execução do ''bloco 4'' não é delegada ao ''sequencer'', mas sim a ''Thread A'' que irá bloquear automaticamente quando o fluxo de execução chegar ao ''bloco 4'' e o resultado do ''bloco 2'' ainda não estiver disponível. Caso o resultado do ''bloco 2'' já esteja disponível, a ''Thread A'' não irá bloquear e seguirá executando o ''bloco 4''. Infelizmente, pode-se não querer o programa bloqueie quando existe a dependência de dados explicita como essa alternativa sugere.

Por isso, a seguir vê-se um diagrama de sequência sobre a implementação de ''future'' descrita no artigo {DIV(type="span")}[{DIV}[#Refer_ncias_|14]] inicial de 1977. Este artigo descreve que, além das tradicionais ''chamadas-por-valor'' e ''chamadas-por-referência'', também existem as ''chamadas-por-future'', onde cada parâmetro da função é ligado a um processo separado (chamado ''future''). Este processo é dedicado a calcular o valor do argumento que a ''future'' representa, o que completamente permite a execução paralela dos argumentos da função, assim aumentado o poder expressivo da linguagem de programação.
::{img fileId="968"}::
::__Fonte:__ Própria::

No exemplo anterior, foi simplificado funcionamento do algoritmo do guarda e abstraiu-se quem é o ''sequencer'' fazendo com que a entidade que representa tipo __Guarda__, chame o método ''do_things()'' como se ele fosse o ''sequencer''. A versão estritamente correta da implementação seria fazer com que uma thread como ''Thread A'' fosse o ''sequencer'', e então  a ''Thread A'' deveria fazer a chamada de ''do_things()''. Além dessa simplificação, assume-se que a implementação de Thread utilizada não inicia a execução imediatamente após sua criação. Ela espera até que o método __start()__ seja chamado, e que a thread permita a execução de vários funções, uma após a outra. As funções que esta thread precisa executar são adicionadas através do método ''append()''.

Os eventos que acontecem no diagrama de sequência são os seguintes, primeiro a ''Thread A'' executa um bloco de código não crítico. Depois ela cria uma ''Future'' com um __job__ que é um ponteiro de função para o ''bloco 2''. O método ''then()'' da variável ''future'' é utilizada para adicionar os blocos de código que são dependentes da seção crítica no ''bloco 2''. Os resultados da execução da seção crítica no ''bloco 2'' são passado em diante para o ''bloco 4'' como parâmetros que o ponteiro de função de entrada do ''bloco 4'' aceita.

Depois de criada a ''future'', a ''Thread A'' chama o método ''guard_vouch()'' passando a ''future'' como parâmetro. Então o algoritmo do guarda segue o fluxo da sua execução como já explicado na seção Guarda. Por simplicidade, assumi-se que depois que o método ''guard_vouch()'' retornou, a ''Thread A'' não assumiu o papel do ''sequencer'', mas que a entidade Guarda do diagrama de sequência é o atual ''sequencer'' em execução, e que já de imediato o ''sequencer'' chamou o método ''do_things()'' da ''future''. Uma vez que o método ''do_things()'' completou sua execução, o ''sequencer'' para de executar o ''bloco 2'', e chama o método ''jobs_list->start()'' que chamada a ''Future Thread'' para realizar a execução dos blocos dependentes da seção crítica. Uma vez que isso acontece, o ''sequencer'' inicia a execução de uma outra seção crítica, que foi omitida no diagrama. Assim, permiti-se que o ''sequencer'' exclusivamente execute seções críticas, enquanto a ''Future Thread'' executa os blocos dependentes da seção crítica, assincronamente, junto com a execução do ''bloco 3'' da ''Thread A''.

Claramente esta implementação mostrada é simples, e trata-se de uma leve modificação da versão original {DIV(type="span")}[{DIV}[#Refer_ncias_|14]]. Seu defeito é sempre realizar a criação de uma nova thread para cada variável ''Future'', pois assim tem-se a criação de muitas threads no sistema, já que sempre cria-se uma thread nova depois que adiciona-se o primeiro bloco com dependência de dados. A implementação inicial {DIV(type="span")}[{DIV}[#Refer_ncias_|14]] dispõe de um serviço de execução que trata de criar novas threads na medida que o sistema necessita. Por exemplo, um sistema embarcado pode não possuir memória suficiente para que muitas thread sejam criadas, então esse serviço de threads limitaria o número máximo de threads que podem ser criadas ao mesmo tempo, e caso esse limite seja ultrapassado, novas requisições aguardam em uma fila, i.e., bloqueiam até que recursos estejam disponíveis para a execução das seções de código dependentes da seção crítica

Um serviço/implementação mais avançado pode automaticamente informar a ''Thread A'' que, existe um bloco de código dependente da seção crítica ainda não executado, e assim, antes que a ''Thread A'' termine sua execução, ela verifica se tal condição é verdadeira, e caso, sim, ela aguarda por este bloco está disponível para execução antes de chamar método ''exit()'' da ''Thread A''. Uma desvantagem é que a ''Thread A'' pode ainda não ter terminado de executar o ''bloco 3'' quando o bloco dependente da seção crítica estiver terminado. Assim, teria-se o atraso da execução do ''bloco 4''. Outra  desvantagem desta alternativa é que o bloco de código dependente pode demorar muito tempo antes que ele possa ser executado, e assim, o sistema desperdiçaria a memória ocupada pela ''Thread A'' que ficou esperando. Assim, a alternativa original de manter um serviço especializado de threads em executar os blocos de código dependentes trás uma melhor vantagem de economia de memória, entretanto ele conta com o overhead de sua manutenção, que conta com criações de novas thread quando a demanda for alta e destruição de threads quando a demanda por blocos dependentes de código for baixa. Mas talvez em uma implementação mais esperta pode-se utilizar a idle thread {DIV(type="span")}[{DIV}[#Refer_ncias_|21]] do sistema para realizar a manutenção de tal serviço, assim reaproveitando recursos do sistema inutilizados.
%%%
!!!!Futures versus Promises  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
A linguagem JavaScript recentemente em 2015, incorporou nativamente a classe ''Promise'' {[https://www.ecma-international.org/ecma-262/6.0/#sec-promise-objects|6]}. Conceitualmente elas são muito similares a ''futures'' {DIV(type="span")}[{DIV}[#Refer_ncias_|14]], mas a implementação de JavaScript é bastante peculiar ao modelo de processamento de JavaScript, que é baseado na existência de uma única thread no sistema {DIV(type="span")}[{DIV}[#Refer_ncias_|16], [#Refer_ncias_|18]]. A seguir vê-se uma breve ilustração da única thread que existe no mundo JavaScript. Essa thread chama-se ''Event loop'', e tudo o que é feito, passa por ela. No caso da implementação das ''promises'', quando uma ''promise'' é resolvida e obtém o seu valor, elas furam {DIV(type="span")}[{DIV}[#Refer_ncias_|16]] a fila de ''callbacks'', para assim serem executadas o mais brevemente possível, preferencialmente mais cedo, ao contrário do modelo usual utilizado para timers, de executar algum tempo depois que o timer expirou, preferencialmente mais tarde pois são adicionados no final da fila.
::{img fileId="945"}::
::__Fonte:__ Referência {DIV(type="span")}[{DIV}[#Refer_ncias_|16]]::

Assim, ''promises'' em JavaScript funcionam não para fazer computações pesadas, mas sim esperar por eventos assíncronos e que podem demorar muito tempo para acontecer, como por exemplo, esperar pela resposta de uma requisição de rede. A seguir vê-se um exemplo do uso de uma ''promise'' que executa assincronamente. Em JavaScript como somente existe uma thread, então tudo o que se executa é sequencialmente {[https://benjaminhorn.io/code/part-2-cpu-intensive-javascript-computations-without-blocking-the-single-thread/|7]}. Na execução de ''promises'' em JavaScript, refere-se assincronamente para dizer que ao se executar este trecho de código, ele não irá bloquear, e somente algum momento mais tarde (assíncrono, fora de sincronia), a soma será realizada, e novamente sem bloquear a execução da única thread que existe, o ''Event loop''.
{CODE(colors="c")}
function sum(xPromise, yPromise) {
    return Promise.all([xPromise, yPromise])
    .then(function(values) {
        return values[0] + values[1];
    } );
}

sum(fetchX(), fetchY())
.then(function(sum) {
    console.log(sum);
});
{CODE}
::__Fonte:__ Referência {DIV(type="span")}[{DIV}[#Refer_ncias_|16]]::

Assuma de os método ''fetchX()'' e ''fetchY()'' returnam alguma ''promise'' depois de fazerem alguma computação. A função ''sum()'' mostrada anteriormente, recebe duas ''promises'' como parâmetro, e chama o método estático ''Primise.all()'', que recebe um array de ''promises'' e retorna uma nova ''promise'' que irá chamar o seu correspondente ''callback'' indicado por ''then()'', assim que todas as suas "''sub-promises''” forem resolvidas, i.e., efetivamente conterem um valor ao invés de somente serem uma promessa de conter algum valor.

Diferente de JavaScript, em C++ e outras linguagens como Java, existem as implementações dos tipos ''Futures'' e ''Promises''. A biblioteca ''std'' do C++ defines as classes template ''std{DIV(type="span")}:{DIV}:promise'' {[https://en.cppreference.com/w/cpp/thread/promise|8]} e ''std{DIV(type="span")}:{DIV}:future'' {[https://en.cppreference.com/w/cpp/thread/future|9]}. A funcionalidade empregada a esses tipos são a transmissão de resultados da computação de uma thread para outra thread que aguarda os resultados. De maneira tradicional, para receber um valor de uma thread precisa-se compartilhar uma variável de condição e um ponteiro comuns a ambas as threads. Uma vez que a outra thread obtém o valor e coloca o mesmo no ponteiro compartilhado, pede-se para a variável de condição liberar a passagem. Então a outra thread que aguardava o resultado desbloqueia e pode seguir com a execução. A desvantagem dessa abordagem é a necessidade de manter e operar diretamente a variável de condição e o ponteiro, e caso queira-se compartilhar mais resultados entre as diferentes threads, a programação fica ainda mais complicada pois precisa-se manter sincronia com mais variáveis de condição. Já com ''std{DIV(type="span")}:{DIV}:promise'' e ''std{DIV(type="span")}:{DIV}:promise'', pode-se abstrair essas operações repetitivas e simplificar a programação. A seguir vê-se um exemplo de utilização destas classes para compartilhamento de dados entre duas threads:
::{img fileId="947"}::
::__Fonte:__ Referência {DIV(type="span")}[{DIV}[#Refer_ncias_|17]]::

No exemplo anterior, tem-se a ''Thread 1'' criando um objeto do tipo ''std{DIV(type="span")}:{DIV}:promise'', então solicitando que o objeto ''std{DIV(type="span")}:{DIV}:promise'' retorne seu objeto do tipo ''std{DIV(type="span")}:{DIV}:future''. Em seguida, cria-se a ''Thread 2'' passando o objeto ''std{DIV(type="span")}:{DIV}:promise''. Uma vez que a ''Thread 1'' tentar acessar o valor dentro de seu objeto ''std{DIV(type="span")}:{DIV}:future'', ela irá bloquear automaticamente caso esse resultado ainda não tenha sido colocado dentro do ''std{DIV(type="span")}:{DIV}:future'' através de seu objeto ''std{DIV(type="span")}:{DIV}:promise'' correspondente. A seguir vê-se um exemplo de código em C++ que segue o padrão descrito no diagrama anterior:
{CODE(colors="c")}
#include <iostream>
#include <thread>
#include <future>

void initiazer(std::promise<int> * promiseObject)
{
    std::cout << "Inside Thread" << std::endl;
    promiseObject->set_value(35);
}

int main()
{
    std::promise<int> promiseObj;
    std::future<int> futureObject = promiseObj.get_future();
    std::thread thread(initiazer, &promiseObj);
    std::cout << futureObject.get() << std::endl;
    thread.join();
    return 0;
}
{CODE}
::__Fonte:__ Referência {DIV(type="span")}[{DIV}[#Refer_ncias_|17]]::

!!!!Futures versus Observables  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Não há muito o que dizer sobre a diferença entre ''futures'' {DIV(type="span")}[{DIV}[#Refer_ncias_|14]] e ''observables'' {DIV(type="span")}[{DIV}[#Refer_ncias_|20]]. Cada um deles servem a propósitos bem específicos, entretanto o domínio de soluções de problemas entre ''futures'' e ''observables'' possuí uma intersecção válida quando não utiliza-se ''futures'' que realizam o bloqueio da thread, mas sim ''futures'' que registram uma lista de blocos de execução dependentes para serem chamados assim que o resultado da future estiver completo. Neste contexto, a versão equivalente para ''observables'' seria registrar a lista de blocos dependentes do resultado como observadores.

A desvantagem de utilizar ''observables'' no lugar de uma ''futures'' seria o encadeamento manual de ''observables'' necessário para cada um dos blocos de dados da cadeia. A desvantagem de utilizar ''futures'' no lugar de ''observables'' seria a impossibilidade de registrar vários blocos como dependentes do mesmo resultado, pois ''observables'' permitem que vários ouvintes sejam registrados e que estes ouvinte seja chamados várias vezes, i.e., a cada vez que um novo resultado é produzido. Enquanto ''futures'' chamam seu ouvinte uma única vez para um único resultado gerado.

!!!Diagrama das Estruturas de Dados Originais  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Para o funcionamento do algoritmo {DIV(type="span")}[{DIV}[#Refer_ncias_|1]], utiliza-se algumas estruturas de dados em "C”. A seguir vê-se as relações entre elas:
{img fileId="908"}

{CODE(colors="c")}
typedef struct
{
    chain_t* next;
} chain_t;

typedef struct
{
    chain_t* head;
    chain_t* tail;
} guard_t;

typedef struct
{
    chain_t* head;
    chain_t* tail;
    sleep_t wait;
} actor_t;
{CODE}

!!!Diagrama de Sequência - Versão original em C  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Exemplo do fluxo de execução de uma única Thread que se torna o ''sequencer'', e executa sua seção crítica:
{img fileId="949"}

A seguir vê-se a implementação do algoritmo do guarda, como apresentado no artigo {DIV(type="span")}[{DIV}[#Refer_ncias_|1]], utilizando a linguagem "C".
{CODE(colors="c")}
void guard_setup(guard_t* self)
{
    self->head = self->tail = NULL;
}

chain_t* guard_vouch(guard_t* self, chain_t* item)
{
    item->next = NULL;
    chain_t* last = FAS(&self->tail, item); // V1
    if (last)
    {
        if (CAS(&last->next, NULL, item)) // V2
            return NULL;
        // last->next == DONE
    }
    self->head = item; // V3
    return item;
}

chain_t* guard_clear(guard_t* self)
{
    chain_t* item = self->head; // C1
    // item != NULL
    chain_t* next = FAS(&item->next, DONE); // C2
    if (!next)
        CAS(&self->tail, item, NULL); // C3
    CAS(&self->head, item, next); // C4
    return next;
}
{CODE}

!!Análise de viabilidade  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
No EPOS x86 não hà uma implementação para a operação FAS, uma das duas primitivas atômicas utilizadas no algoritmo das guards. Uma possível implementação para essa operação é apresentada a seguir:

{CODE(colors="c")}
template<typename T>
static T fas(volatile T & value, volatile T replacement) {
    ASM("lock xchg %0, %2" : "=a"(replacement) : "a"(replacement), "m"(value) : "memory");
    return replacement;
}
{CODE}
Mesmo que não consiga-se realizar uma implementação correta para ''FAS()'', pode-se implementar a FAS utilizando CAS. Por exemplo, uma chamada FAS seria ''FAS(entrada, saída)'' e a versão com CAS equivalente seria ''CAS(entrada, entrada, saída)''. Assim, a diferença seria que utilizar um CAS pode ser menos eficiente ao invés de utilizar somente um ''FAS()'' implementado em assembly.

Como ARM não suporta CAS, apenas LC/SC, pretende-se limitar a implementação do algoritmo apenas à versão do EPOS para a arquitetura x86.

A seguir vê-se um simples programa que utiliza a operação ''CAS()'' atualmente implementada no EPOS.

::__Listagem 4:__ cas_test.cc::
{CODE(colors="c")}
// EPOS CAS Component Test Program

#include <utility/ostream.h>
#include <architecture/ia32/cpu.h>

using namespace EPOS;
OStream cout;

int main()
{
    cout << endl << "Welcome to the CPU::cas() instruction test!" << endl;
    int original = 5;
    int compare = 5;
    int replacement = 6;
    int replaced;

    cout << "original=" << original
            << ", compare=" << compare
            << ", replacement=" << replacement
            << ", replaced=" << replaced
            << endl;

    replaced = CPU::cas(original, compare, replacement);

    cout << "original=" << original
            << ", compare=" << compare
            << ", replacement=" << replacement
            << ", replaced=" << replaced
            << endl;

    cout << "The CPU::cas() instruction set ran successfully!" << endl << endl;
}
{CODE}

__Resultado da execução de cas_test.cc__
{CODE()}
Welcome to the CPU::cas() instruction test!
original=5, compare=5, replacement=6, replaced=0
original=6, compare=5, replacement=6, replaced=5
The CPU::cas() instruction set ran successfully!

The last thread has exited!
Rebooting the machine ...
{CODE}

!!Implementação  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Código disponível em svn:
# https://github.com/evandrocoan/predictable_synchronisation_algorithms_for_asynchronous_critical_sections
# https://epos.lisha.ufsc.br/svn/makers/predictable_synchronisation_algorithms_for_asynchronous_critical_sections
Para a implementação do algoritmo das guardas no EPOS foram definidas três novas classes, que representam as principais abstrações envolvidas na versão básica do algoritmo, seções críticas, elementos de guarda e a guarda em si. Essas classes encapsulam toda a lógica do algoritmo e fornecem interfaces para a delegação da execução de seções críticas por threads arbitrárias. 

Neste capítulo será descrito como ocorreu o processo de implementação das novas classes que implementam o algoritmo das guardas, assim como os testes realizados para validação da implementação.

!!!Closure [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Assim como ocorre com os algoritmos de sincronização tradicionais, uma das bases dos algoritmos de sincronização baseados em delegação é o conceito de seções críticas. Dessa forma, foi definida uma nova classe para representar essas entidades. 

A classe que representa seções críticas  foi criada com base na classe Function_Handler e recebeu o nome de Closure. Essa classe possui apenas dois atributos, _handler e _link. O atributo _handler representa a função relacionada à seção crítica e o atributo _link representa um elemento de lista simples. Além desses dois atributos, a classe Closure também implementa o método público run(), que é utilizado pelo sequencer para executar uma seção crítica, que na verdade é o mesmo trabalho realizado pelo operator “()”, que deixava o código pouco intuitivo. A Listagem 5 exibe o código da classe Closure.

O construtor da classe Closure recebe um objeto representando uma função, que deve ser compatível com o tipo Function, definido no arquivo handler.h, como parâmetro. Além disso, o construtor cria o _link relativo à seção crítica sendo construída, que será utilizado para adição e remoção de seções críticas de estruturas de dados, da mesma forma como acontece com objetos da classe Thread. 

::__Listagem 5:__ Seção crítica::
{CODE(colors="c")}
class Closure
{
public:
   friend class Guard;
   typedef Handler::Function Function;
   typedef List_Elements::Singly_Linked<Closure> Element;

public:
   Closure(Function * h): _handler(h), _link(this) {}
   ~Closure() {}

   void operator()() { _handler(); }
   void run()        { _handler(); } // Alias for ()

private:
   Function * _handler;
   Element _link; // Inspired by the thread code
};
{CODE}
Exemplos da utilização da classe Closure durante a submissão de seções críticas à guarda estão na seção sobre os testes realizados.

!!!Element [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Como elementos da guarda são elementos de lista simples, com apenas um nível de encadeamento, optou-se por utilizar a classe List_Elements::Singly_Linked, Listagem 6, definida no arquivo list.h, para representá-los. 

No entanto, foi necessário adicionar uma relação de amizade entre a classe Singly_Linked e a classe Guard, para que as operações CAS e FAS dos métodos vouch e clear da classe guardam possam acessar diretamente o atributo _next do elemento de lista. Isso porque, não é possível efetuar as operações CAS e FAS utilizando getters.     

::__Listagem 6:__ Link contendo a seção crítica::
{CODE(colors="c")}
   // Simple List Element
   template<typename T>
   class Singly_Linked
   {
   public:
       friend class _UTIL::Guard;
       typedef T Object_Type;
       typedef Singly_Linked Element;

   public:
       Singly_Linked(const T * o): _object(o), _next(0) {}

       T * object() const { return const_cast<T *>(_object); }

       Element * next() const { return _next; }
       void next(Element * e) { _next = e; }

   private:
       const T * _object;
       Element * _next;
   };
{CODE}

!!!Guard [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
O código onde da classe Guard, definida no arquivo utility/guard.h, é mostrado na Listagem 7. Essa classe implementa a estrutura da guarda descrita no capítulo de fundamentação teórica. 

::__Listagem 7:__ Definição da interface do Guarda::
{CODE(colors="c")}
class Guard
{
public:
   typedef Closure::Element Element;

private:
   static const int NULL = 0;
   static const int DONE = 1;

public:
   Guard();
   ~Guard();

   void submit(Closure * cs);
   Element * vouch(Element * item);
   Element * clear();

private:
   Element * _head;
   Element * _tail;
};
{CODE}
Além das operações vouch, clear e a operação de criação da guarda (setup), já apresentadas no capítulo de fundamentação teórica, a classe guarda implementa uma nova operação, chamada de submit. Essa operação encapsula o protocolo de submissão de seções críticas à guarda e pode ser utilizada por threads para a delegação da execução de seções críticas. A Listagem 8 apresenta o construtor e destrutor da guarda, assim como a implementação do método submit().

::__Listagem 8:__ Nova convenção para a submissão de requisições de execução de seções críticas::
{CODE(colors="c")}
// Object Methods
Guard::Guard(): _head(0), _tail(0)
{
   db<Synchronizer>(TRC)   << "Guard(head=" << _head
                           << ", tail=" << _tail
                           << ") => " << this << endl;
}

Guard::~Guard()
{
   db<Synchronizer>(TRC) << "~Guard(this=" << this << ")" << endl;
}

void Guard::submit(Closure * cs)
{
   Element * cur = vouch(&(cs->_link));
   if (cur != NULL) do {
       cur->object()->run();
       cur = clear();
   } while (cur != NULL);   
}
{CODE}
A implementação dos métodos vouch(...) e clear são apresentadas na Listagem 8. Essas implementações não diferem muito dos códigos apresentados no capítulo de fundamentação teórica, lembrando que a operação vouch é responsável por adicionar novos elementos à guarda e sinalizar threads quando essas precisarem assumir o papel de sequencer, e a operação clear é responsável pela remoção de elementos da guarda e por sinalizar ao sequencer quando a guarda estiver vazia. 

Uma adição importante foi a deleção de seções críticas que já foram executadas. Essa deleção deve geralmente ocorrer durante a execução do método clear. No entanto, existe uma situação excepcional, que pode ocorrer quando a guarda possui apenas um elementos e operações vouch e clear executam simultaneamente. Nesse caso, a passagem do papel de sequencer da thread executando clear para a thread executando vouch. Quando isso acontecer, o sequencer executando vouch precisará também deletar o elemento da guarda previamente removido da lista pela operação clear.

::__Listagem 9:__ Implementações de vouch() e clear()::
{CODE(colors="c")}
Guard::Element * Guard::vouch(Element * item)
{
   db<Synchronizer>(TRC) << "Guard::vouch(this=" << this << " head= " << _head << " tail= " << _tail <<  ")" << endl;
   item->next(NULL);
   Element * last = CPU::fas(_tail, item);
   if (last){
       if (CPU::cas(last->_next, reinterpret_cast<Element *>(NULL), item) == NULL)
           return NULL;
       delete item->object();
   }
   _head = item;       
   return item;
}

Guard::Element * Guard::clear()
{
   db<Synchronizer>(TRC) << "Guard::clear(this=" << this << " head= " << _head << " tail= " << _tail <<  ")" << endl;
   Element * item = _head;
   Element * next = CPU::fas(item->_next, reinterpret_cast<Element *>(DONE));
   bool mine = true;
   if (!next)
       mine = CPU::cas(_tail, item, reinterpret_cast<Element *> (NULL)) == item;
   CPU::cas(_head, item, next);
   if (mine)
       delete item->object();
   return next;   
}
{CODE}

!!!FAS [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
A operação FAS, uma das duas primitivas atômicas utilizados no algoritmo das guardas, ainda não havia sido implementada no EPOS, Dessa forma, foi adicionada ao arquivo /include/archtecture/ia_32/cpu.h, onde estão implementadas as primitivas atômicas de sincronização no EPOS, uma nova implementação para FAS.

O código de FAS foi inspirado no código das outras operações atômicas e nas descrições das instruções atômicas nos manuais da Intel.

Além disso, assim como ocorre com as outras operações atômicas, foi adicionada uma implementação em software, independente de arquitetura, no arquivo /include/cpu.h. No entanto, como essa operação não possui suporte de hardware, ela não deve provê garantias de atomicidade.

::__Listagem 10:__ Implementação do FAS::
{CODE(colors="c")}
// /include/archtecture/ia_32/cpu.h
template<typename T>
static T fas(volatile T & value, volatile T replacement) 
{
    ASM("lock xchg %0, %2" : "=a"(replacement) : "a"(replacement), "m"(value) : "memory");
    return replacement;
}

// /include/cpu.h
static int fas(volatile int & value, volatile int & replacement) {
     int old = value;
     value = replacement;
     return old;
 }
{CODE}

!!!Futures [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Das 2 implementações distintas de futures apresentadas, realizamos a implementação do modelo de future bloqueante, apresentado no artigo referência {DIV(type="span")}[{DIV}[#Refer_ncias_|2]]. Como a implementação atual do algoritmo do guarda não suporta passagens de parâmetros variádicos, manualmente fez-se a implementação dos métodos para que aceitem vários parâmetros com meta programação. Ver a seção [#Closure_Metaprogramada|Closure Metaprogramada]. 

Uma vez que a future é resolvida, i.e., seu método ''resolve()'' é chamado e seu valor é definido, ele não pode ser mais alterado, e qualquer um que tentar obter o valor da future com ''get_value()'' não precisará mais bloquear, pois o valor não é indefinido. Futures são implementadas totalmente no arquivo header ''.h'' por que são meta classes que são instanciadas pelo compilador e seu código é gerado somente caso alguém crie uma especialização com algum tipo de dado.
::__Listagem XX:__ Implementação de future em ''/include/utility/future.h''::
{CODE(colors="c")}
// EPOS Guard Component Declarations

#ifndef __future_h
#define __future_h

#include <condition.h>

__BEGIN_UTIL

template<typename FutureType>
class Future 
{
public:
    Future(): _condition(), _is_resolved() {
        db<Synchronizer>(WRN)   << "Future(_is_resolved=" << _is_resolved 
                                << ", _condition=" << _condition.size()
                                << ") => " << this << endl;
    }

    ~Future() {
        db<Synchronizer>(WRN) << "~Future(this=" << this << ")" << endl;
    }

    FutureType get_value() {
        db<Synchronizer>(WRN) << "Future::get_value(this=" << this 
                              << " _is_resolved=" << _is_resolved 
                              << " _condition=" << _condition.size()
                              <<  ")" << endl;
        if(!_is_resolved) _condition.wait();
        return _value;
    }

    void resolve(FutureType value) { 
        db<Synchronizer>(WRN) << "Future::resolve(this=" << this 
                              << " _is_resolved=" << _is_resolved 
                              << " _condition=" << _condition.size()
                              <<  ")" << endl;
        assert(!_is_resolved);
        // If `resolve()` was called and the instruction pointer got until here, and the thread
        // is unscheduled, and another thread call `resolve()`, then, the `assert` above will 
        // not work.
        _value = value;
        _is_resolved = true;
        _condition.broadcast();
    }

private:
    bool _is_resolved;
    FutureType _value;
    Condition _condition;
};

__END_UTIL

#endif
{CODE}

Agora vemos um simples programa que demonstra o uso de futures:
::__Listagem 11:__ Arquivo de Teste ''/app/future_simple_test.cc''::
{CODE(colors="c")}
// EPOS Synchronizer Component Test Program

#include <thread.h>
#include <utility/future.h>
#include <semaphore.h>
#include <alarm.h>

using namespace EPOS;

Semaphore display_lock;

// #define log(argument) display_lock.p(); db<Synchronizer>(WRN) << argument; display_lock.v();
#define log(argument) db<Synchronizer>(WRN) << argument;

int producerFunction(Future<int>* future) {
    log( "producerFunction ()" << endl )
    Delay thinking(1000000);
    future->resolve(10);

    log( "producerFunction (resolving future=" << future << " to 10)" << endl )
    return 0;
}

int consumerFunction(Future<int>* future) {
    log( "consumerFunction ()" << endl )
    log( "consumerFunction (result=" << future->get_value() << ")" << endl )
    log( "consumerFunction (result=" << future->get_value() << ")" << endl )
    return 0;
}

int main()
{
    log( endl << "Starting main application..." << endl )
    Future<int>* future = new Future<int>();

    Thread* consumer = new Thread(&consumerFunction, future);
    Thread* producer = new Thread(&producerFunction, future);

    consumer->join();
    producer->join();

    log( "Exiting main application..." << endl )
    return 0;
}
{CODE}

::__Listagem 12:__ Resultado da execução de ''/app/future_simple_test.cc''::
{CODE(colors="c")}
Starting main application...
Future(_is_resolved=0, _condition=0) => 0x00097f5c
consumerFunction ()
Future::get_value(this=0x00097f5c _is_resolved=0 _condition=0)
producerFunction ()
Future::resolve(this=0x00097f5c _is_resolved=0 _condition=1)
consumerFunction (result=10)
Future::get_value(this=0x00097f5c _is_resolved=1 _condition=0)
consumerFunction (result=10)
producerFunction (resolving future=0x00097f5c to 10)
Exiting main application...
{CODE}

A seguir vemos um simples exemplo de uso de futures. Nesse exemplo, evolvemos varias classes to EPOS, fazendo seu uso em conjunto com o algoritmo do guarda.
::__Listagem XX:__ Arquivo de Teste ''/app/future_guard_test.cc''::
{CODE(colors="c")}
// EPOS Synchronizer Component Test Program

#include <thread.h>
#include <utility/guard.h>
#include <utility/future.h>
#include <semaphore.h>
#include <alarm.h>

using namespace EPOS;
static volatile int counter = 0;

Guard guard;
Semaphore display_lock;

// #define log(argument) display_lock.p(); db<Synchronizer>(WRN) << argument; display_lock.v();
#define log(argument) db<Synchronizer>(WRN) << argument;

void increment_counter(Future<int>* future) {
    Delay thinking(1000000);
    counter = counter + 1;
    log( "increment_counter (counter=" << counter << ")" << endl )
    future->resolve(counter);
}

int functionA() {
    log( "functionA ()" << endl )
    Future<int>* future = new Future<int>();

    guard.submit(&increment_counter, future);
    log( "functionA (result=" << future->get_value() << ")" << endl )    return 0;
}

int functionB() {
    log( "functionB ()" << endl )
    Future<int>* future = new Future<int>();

    guard.submit(&increment_counter, future);
    log( "functionB (result=" << future->get_value() << ")" << endl )
    return 0;
}

int main()
{
    log( endl << "Starting main application..." << endl )

    Thread* producer = new Thread(&functionA);
    Thread* consumer = new Thread(&functionB);

    consumer->join();
    producer->join();

    log( "Exiting main application..." << endl )
    return 0;
}
{CODE}

::__Listagem 12:__ Resultado da execução de ''/app/future_guard_test.cc''::
{CODE(colors="c")}
Starting main application...
functionA ()
Future(_is_resolved=0, _condition=0) => 0x0008fef4
functionB ()
Future(_is_resolved=0, _condition=0) => 0x0008feb4
Future::get_value(this=0x0008feb4 _is_resolved=0 _condition=0)
increment_counter (counter=1)
Future::resolve(this=0x0008fef4 _is_resolved=0 _condition=0)
increment_counter (counter=2)
Future::resolve(this=0x0008feb4 _is_resolved=0 _condition=1)
functionB (result=2)
Future::get_value(this=0x0008fef4 _is_resolved=1 _condition=0)
functionA (result=1)
Exiting main application...
{CODE}

!!Testes [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Criamos um arquivo de testes baseado no exemplo de http://pages.cs.wisc.edu/~remzi/OSTEP/threads-intro.pdf.
# Os exemplos com o EPOS multicore, com duas CPUS, variavelmente gerava exceções, que não ocorrem na versão single core. Não pode-se identificar exatamente qual a causa dessas exceções, que são do tipo GPF e PF. Uma constatação importante é que essas exceções ocorrem mesmo em versões minimamente modificadas das aplicações de testes padrões do EPOS.
# Por duas vezes conseguimos fazer a contagem falhar, ambas vezes utilizando o critério de escalonamento RR. No entanto, nas últimas muitas execuções a contagem voltou a ocorrer corretamente, o que é indesejado, mesmo com a utilização do escalonamento RR. Com duas CPUS.
# Foram definidos três programas de teste para a aplicação contadora simples: uma sem sincronização, uma utilizando semáforos e outra com guardas.
Todos os testes a seguir foram realizados utilizando ''static const unsigned int CPUS = 4''.

!!!fas_test.cc [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Aqui testa-se a implementação nova de FAS feita, repetidamente fazendo a troca de variáveis em duas thread executando em CPUs diferentes.

::__Listagem 11:__ Arquivo de Teste ''/app/fas_test.cc''::
{CODE(colors="c")}
// EPOS Synchronizer Component Test Program

#include <utility/ostream.h>
#include <semaphore.h>
#include <thread.h>
#include <machine.h>
#include <cpu.h>

using namespace EPOS;

Semaphore display_lock;
// #define log(argument) display_lock.p(); db<Synchronizer>(WRN) << argument; display_lock.v();
#define log(argument) db<Synchronizer>(WRN) << argument;

static const int iterations = 1e5;

int old = 0;
int current = 10;
int next = 11;

#define check(thread, name) \
    log( thread << name \
            << ", old=" << old \
            << ", current=" << current \
            << ", next=" << next \
            << endl )

int myThread1() {
    check("Thread 1", ", begin")
    
    for (int i = 0; i < iterations; i++) 
    {
        old = CPU::fas(current, next);
        current = CPU::fas(next, old);
        next = CPU::fas(old, current);
        // check("Thread 1", ", now")
    }

    check("Thread 1", ", result")
    return 0;
}

int myThread2() {
    check("Thread 2", ", begin")

    for (int i = 0; i < iterations; i++) {
        old = CPU::fas(current, next);
        current = CPU::fas(next, old);
        next = CPU::fas(old, current);
        // check("Thread 2", ", now")
    }

    check("Thread 2", ", result")
    return 0;
}

int main()
{
    log( "iterations=" << iterations << endl )
    Thread p1(&myThread1);
    Thread p2(&myThread2);

    check("Thread 0", ", main")
    p1.join();
    p2.join();

    check("Thread 0", ", end")
    return 0;
}
{CODE}

::__Listagem 12:__ Resultado da execução de ''/app/fas_test.cc''::
{CODE(colors="c")}
<1>: MMU is disabled! :<1>
<2>: MMU is disabled! :<2>
<3>: MMU is disabled! :<3>
<0>: iterations=100000 :<0>
<0>: Thread 0, main, old=0, current=10, next=11 :<0>
<2>: Thread 1, begin, old=0, current=10, next=11 :<2>
<1>: Thread 2, begin, old=10, current=10, next=11 :<1>
<2>: Thread 1, result, old=10, current=10, next=11 :<2>
<1>: Thread 2, result, old=10, current=10, next=11 :<1>
<1>: Thread 0, end, old=10, current=10, next=11 :<1>
<0>: The last thread has exited! :<0>
<0>: Rebooting the machine ... :<0>
{CODE}

!!!count_sync_guard.cc [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Neste teste, utiliza-se a implementação do guarda para controlar o acesso a seção crítica ''increment_counter()'', que simplesmente soma um contador. No final da execução, espera-se que resultado da conta seja precisamente o número de iterações feita.

::__Listagem 13:__ Arquivo de teste ''/app/count_sync_guard.cc''::
{CODE(colors="c")}
// EPOS Synchronizer Component Test Program

#include <thread.h>
#include <machine.h>
#include <utility/guard.h>
#include <alarm.h>

using namespace EPOS;

static volatile int counter = 0;
static const int iterations = 1e4;

Guard counter_guard;
Guard display_guard;
Thread * pool[5];

void a_begin() {
    db<Synchronizer>(WRN)   << "A: begin (counter = " << counter 
                            << ")" << endl;
}

void b_begin() {
    db<Synchronizer>(WRN)   << "B: begin (counter = " << counter 
                            << ")" << endl;
}

void c_begin() {
    db<Synchronizer>(WRN)   << "C: begin (counter = " << counter 
                            << ")" << endl;
}

void d_begin() {
    db<Synchronizer>(WRN)   << "D: begin (counter = " << counter 
                            << ")" << endl;
}

void e_begin() {
    db<Synchronizer>(WRN)   << "E: begin (counter = " << counter 
                            << ")" << endl;
}

void increment_counter(){
    counter = counter + 1;
    // db<Synchronizer>(WRN)   << "increment_counter (counter = " << counter 
    //                         << ")" << endl;
}

int mythread(int arg) {
    switch(arg) {
        case 1:
            display_guard.submit(new Closure(&a_begin));
            break;
        case 2:
            display_guard.submit(new Closure(&b_begin));
            break;
        case 3:
            display_guard.submit(new Closure(&c_begin));
            break;        
        case 4:
            display_guard.submit(new Closure(&d_begin));
            break;
        case 5:
            display_guard.submit(new Closure(&e_begin));
            break;
    }

    for (int i = iterations; i > 0 ; i--) {
        counter_guard.submit(new Closure(&increment_counter));
    }

    return 0;
}

int main()
{
    db<Synchronizer>(WRN)   << "main: begin (counter = " << counter 
                            << ")" << endl;

    pool[0] = new Thread(&mythread, 1);
    pool[1] = new Thread(&mythread, 2);
    pool[2] = new Thread(&mythread, 3);
    pool[3] = new Thread(&mythread, 4);
    pool[4] = new Thread(&mythread, 5);

    db<Synchronizer>(WRN)   << "main: start joining the threads (counter = " << counter 
                            << ")" << endl;

    // join waits for the threads to finish
    for(int i = 0; i < 5; i++) {
        pool[i]->join();
    }

    db<Synchronizer>(WRN)   << "main: done with both (counter = " << counter 
                            << ")" << endl;

    for(int i = 0; i < 5; i++)
        delete pool[i];

    db<Synchronizer>(WRN)   << "main: exiting (counter = " << counter 
                            << ")" << endl;

    return 0;
}
{CODE}

::__Listagem 14:__ Resultado do teste ''/app/fas_test.cc''::
{CODE(colors="c")}
<1>: MMU is disabled! :<1>
<2>: MMU is disabled! :<2>
<3>: MMU is disabled! :<3>
<0>: main: begin (counter = 0) :<0>
<1>: A: begin (counter = 0) :<1>
<3>: B: begin (counter = 296) :<3>
<2>: C: begin (counter = 1246) :<2>
<0>: D: begin (counter = 6173) :<0>
<1>: main: start joining the threads (counter = 7532) :<1>
<2>: E: begin (counter = 8798) :<2>
<1>: main: done with both (counter = 50000) :<1>
<1>: main: exiting (counter = 50000) :<1>
<0>: The last thread has exited! :<0>
<0>: Rebooting the machine ... :<0>
{CODE}

!!!count_sync_uncoordinated.cc [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
O que se espera deste teste, ao contrário do teste anterior, é que a contagem final de errada, pois nenhuma primitiva de sincronização é utilizada, assim teremos que a operação de incremento, que não é atômica, seja interrompida pelo metade, e no fim o resultado fique errado.

::__Listagem 15:__ Arquivo de teste ''/app/count_sync_uncoordinated.cc''::
{CODE(colors="c")}
// EPOS Synchronizer Component Test Program

#include <semaphore.h>
#include <thread.h>
#include <machine.h>
#include <alarm.h>

using namespace EPOS;

int counter = 0;

Semaphore display_lock;
Semaphore counter_lock; 
#define log(argument) display_lock.p(); db<Synchronizer>(WRN) << argument; display_lock.v();

// mythread()
// Simply adds 1 to counter repeatedly, in a loop
// No, this is not how you would add 10,000,000 to
// a counter, but it shows the problem nicely.
int mythread(char arg) {
    log( arg << ": begin : " << endl)

    for (int i = 0; i < 1e6; i++) {
        if (counter%100000 == 0){
            log( arg << " : " << counter << endl )
        }
        counter = counter + 1;
        // log( "Counting " << counter << endl )
    }

    log( arg << ": done" << endl )
    return 0;
}

// main()
// Just launches two threads (pthread_create)
// and then waits for them (pthread_join)
int main()
{
    log( "main: begin (counter = " << counter << ")" << endl )

    Thread * p1 = new Thread(&mythread, 'A');
    Thread * p2 = new Thread(&mythread, 'B');
    Thread * p3 = new Thread(&mythread, 'C');
    Thread * p4 = new Thread(&mythread, 'D');
    Thread * p5 = new Thread(&mythread, 'E');
    Thread * p6 = new Thread(&mythread, 'F');

    // join waits for the threads to finish
    p1->join();
    p2->join();
    p3->join();
    p4->join();
    p5->join();
    p6->join();

    log( "main: done with both (counter = " << counter << ")"<< endl )
    return 0;
}
{CODE}

::__Listagem 16:__ Resultado do teste ''/app/count_sync_uncoordinated.cc''::
{CODE(colors="c")}
<1>: MMU is disabled! :<1>
<2>: MMU is disabled! :<2>
<3>: MMU is disabled! :<3>
<0>: main: begin (counter = 0) :<0>
<2>: A: begin :  :<2>
<2>: A : 0 :<2>
<1>: B: begin :  :<1>
<3>: C: begin :  :<3>
<0>: D: begin :  :<0>
<2>: E: begin :  :<2>
<1>: F: begin :  :<1>
<1>: F : 100000 :<1>
<2>: E : 100000 :<2>
<3>: A : 100000 :<3>
<0>: B : 100000 :<0>
<2>: C : 100000 :<2>
<1>: D : 100000 :<1>
<0>: F : 200000 :<0>
<3>: C : 200000 :<3>
<2>: E : 200000 :<2>
<3>: A : 200000 :<3>
<0>: B : 200000 :<0>
<1>: D : 200000 :<1>
<3>: E : 300000 :<3>
<0>: F : 300000 :<0>
<2>: A : 300000 :<2>
<2>: A : 300000 :<2>
<3>: C : 300000 :<3>
<2>: E : 400000 :<2>
<1>: F : 400000 :<1>
<1>: D : 345974 :<1>
<1>: B : 400000 :<1>
<3>: D : 346916 :<3>
<0>: F : 385967 :<0>
<0>: E : 393666 :<0>
<2>: A : 400000 :<2>
<1>: C : 477901 :<1>
<0>: F : 498945 :<0>
<3>: D : 446608 :<3>
<1>: E : 476901 :<1>
<0>: B : 484604 :<0>
<3>: C : 500000 :<3>
<1>: E : 528187 :<1>
<0>: D : 525792 :<0>
<2>: A : 571028 :<2>
<2>: F : 600000 :<2>
<1>: E : 567826 :<1>
<3>: A : 654100 :<3>
<1>: B : 588204 :<1>
<1>: E : 600000 :<1>
<3>: D : 689233 :<3>
<0>: B : 700000 :<0>
<0>: A : 600000 :<0>
<0>: D : 646539 :<0>
<3>: F : 658486 :<3>
<1>: C : 691064 :<1>
<0>: D : 700000 :<0>
<0>: D : 700000 :<0>
<1>: C : 700000 :<1>
<3>: F : 700000 :<3>
<2>: A : 700000 :<2>
<3>: B : 700000 :<3>
<0>: E : 700000 :<0>
<0>: C : 800000 :<0>
<3>: A : 800000 :<3>
<0>: D : 800000 :<0>
<1>: F : 817259 :<1>
<2>: B : 800000 :<2>
<2>: D : 900000 :<2>
<2>: D : 800000 :<2>
<3>: E : 800000 :<3>
<0>: C : 900000 :<0>
<2>: A : 900000 :<2>
<3>: F : 900000 :<3>
<2>: B : 900000 :<2>
<0>: D : 900000 :<0>
<3>: C : 1000000 :<3>
<2>: F : 1000000 :<2>
<3>: E : 900000 :<3>
<1>: A : 1000000 :<1>
<1>: A : 1000000 :<1>
<0>: B : 1000000 :<0>
<3>: F : 1100000 :<3>
<3>: C : 1100000 :<3>
<0>: D : 1000000 :<0>
<1>: E : 1000000 :<1>
<2>: B : 1100000 :<2>
<3>: C : 1100000 :<3>
<0>: A: done :<0>
<1>: F: done :<1>
<3>: B: done :<3>
<0>: C: done :<0>
<1>: E: done :<1>
<2>: D : 1100000 :<2>
<2>: D: done :<2>
<2>: main: done with both (counter = 1106138) :<2>
<0>: The last thread has exited! :<0>
<0>: Rebooting the machine ... :<0>
{CODE}

!!!count_sync_semaphore.cc [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
Neste teste, utilizamos os mecanismo de sincronização do semáforo, e ao contrário do teste anterior ''count_sync_uncoordinated.cc'', deste teste espera-se que a contagem seja correta, pois utilizamos o mecanismo de sincronização do semáforo para garantir a atomicidade da operação de adição.

::__Listagem 17:__ Arquivo de teste ''/app/count_sync_semaphore.cc''::
{CODE(colors="c")}
// EPOS Synchronizer Component Test Program

#include <utility/ostream.h>
#include <semaphore.h>
#include <thread.h>
#include <machine.h>

using namespace EPOS;

int counter = 0;

Semaphore display_lock;
Semaphore counter_lock; 
#define log(argument) display_lock.p(); db<Synchronizer>(WRN) << argument; display_lock.v();

// mythread()
// Simply adds 1 to counter repeatedly, in a loop
// No, this is not how you would add 10,000,000 to
// a counter, but it shows the problem nicely.
int mythread(char arg) {
    log( arg << ": begin" << endl )

    for (int i = 0; i < 1e4; i++) {
        counter_lock.p();
        counter = counter + 1;
        counter_lock.v();
    }

    log( arg << ": done" << endl )
    return 0;
}

// main()
// Just launches two threads (pthread_create)
// and then waits for them (pthread_join)
int main()
{
    log( "main: begin (counter = " << counter << ")" << endl )
    Thread * p1 = new Thread(&mythread, 'A');
    Thread * p2 = new Thread(&mythread, 'B');

    // join waits for the threads to finish
    p1->join();
    p2->join();
    log( "main: done with both (counter = " << counter << ")"<< endl )

    delete p1;
    delete p2;
    return 0;
}
{CODE}

::__Listagem 18:__ Resultado do teste ''/app/count_sync_semaphore.cc''::
{CODE(colors="c")}
<1>: MMU is disabled! :<1>
<2>: MMU is disabled! :<2>
<3>: MMU is disabled! :<3>
<0>: main: begin (counter = 0) :<0>
<0>: A: begin :<0>
<0>: B: begin :<0>
<3>: B: done :<3>
<3>: A: done :<3>
<3>: main: done with both (counter = 20000) :<3>
<0>: The last thread has exited! :<0>
<0>: Rebooting the machine ... :<0>
{CODE}

!!!O que aprendemos com os testes [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
# Executar aplicações com mais de 1 CPU ativa variavelmente gera exceções (GPF). Isso acontece mesmo com a aplicação do jantar dos filósofos, se os delays forem removidos.
# Quando requisições são enviadas ao sequencer em uma taxa maior do que ele consegue executar, a memória enche e exceções (PF) ocorrem.
# Com o KVM não funciona o relógio e o escalonador não era chamado.
# Sem o RR fica difícil gerar erros de sincronização.
# Após a primeira execução, o qemu fica muito comportado e para de gerar erros de contagem durante a execução do arquivo corrente mais simples: count_sync_uncoordinated.cc
# Compilando o EPOS utilizado/entregado no trabalho anterior de 2017.2 com GCC 7.2.0, não executa a aplicação principal. Baixamos o código deles, e compilamos. A seguir tem um log completo no modo histérico da execução, mostrando o problema: https://gist.github.com/evandrocoan/3159d9bf1c53620e2e01169d7a4b8d92

!!Referências de Implementação  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
#https://stackoverflow.com/questions/18847424/c-create-custom-function-dispatcher-from-variadic-template
#https://stackoverflow.com/questions/42047795/order-of-parameter-pack-expansion
#https://stackoverflow.com/questions/29194858/order-of-function-calls-in-variadic-template-expansion
#https://stackoverflow.com/questions/43553585/how-to-transform-a-variadic-template-argument-to-another-types-for-calling-anoth
#https://stackoverflow.com/questions/12030538/calling-a-function-for-each-variadic-template-argument-and-an-array
#https://stackoverflow.com/questions/687490/how-do-i-expand-a-tuple-into-variadic-template-functions-arguments/12650100#12650100
#https://stackoverflow.com/questions/7858817/unpacking-a-tuple-to-call-a-matching-function-pointer
#https://en.cppreference.com/w/cpp/language/list_initialization#Notes
#https://stackoverflow.com/questions/47207621/build-function-parameters-with-variadic-templates
#https://stackoverflow.com/questions/16868129/how-to-store-variadic-template-arguments
#https://stackoverflow.com/questions/43026550/how-to-define-a-class-that-can-save-variadic-template-arguments
#https://stackoverflow.com/questions/17996003/how-to-save-variable-number-of-arguments-using-variadic-template-arguments
#https://stackoverflow.com/questions/29220248/is-it-possible-to-store-variadic-arguments-into-a-member-variable
#https://stackoverflow.com/questions/4691657/is-it-possible-to-store-a-template-parameter-pack-without-expanding-it
#https://arne-mertz.de/2016/11/modern-c-features-variadic-templates/
#https://gitlab.com/evandro-crr/epos2/tree/master
#https://stackoverflow.com/questions/13108663/storing-functions-call-and-list-of-parameters-to-invoke-later
#https://stackoverflow.com/questions/4926433/saving-function-pointerarguments-for-later-use
#https://appuals.com/fix-cannot-execute-binary-file-exec-format-error-ubuntu/
#https://stackoverflow.com/questions/30926764/extracting-function-argument-types-as-a-parameter-pack
#https://stackoverflow.com/questions/24948277/unpacking-arguments-of-a-functional-parameter-to-a-c-template-class
#https://stackoverflow.com/questions/14783876/c-is-it-possible-to-extract-class-and-argument-types-from-a-member-function
#https://stackoverflow.com/questions/34836104/how-to-extract-a-selected-set-of-arguments-of-a-variadic-function-and-use-them-t
#https://stackoverflow.com/questions/28033251/can-you-extract-types-from-template-parameter-function-signature
#https://stackoverflow.com/questions/11056714/c-type-traits-to-extract-template-parameter-class
#https://stackoverflow.com/questions/32674839/variadic-member-function-of-template-class
#https://stackoverflow.com/questions/49217891/class-member-function-in-variadic-template
#https://stackoverflow.com/questions/50316284/how-to-achieve-variadic-virtual-member-function
#https://stackoverflow.com/questions/15599679/class-member-function-pointer
#https://stackoverflow.com/questions/40855835/how-do-you-typedef-a-function-pointer-type-with-parameter-pack-arguments
#https://stackoverflow.com/questions/8915797/calling-a-function-through-its-address-in-memory-in-c-c
#https://stackoverflow.com/questions/47005664/call-function-by-known-address-c
#https://en.cppreference.com/w/cpp/language/parameter_pack
#https://stackoverflow.com/questions/47037395/forward-types-in-variadic-template-as-values-references-according-to-function-si
#https://eli.thegreenplace.net/2014/perfect-forwarding-and-universal-references-in-c
#https://stackoverflow.com/questions/3836648/structure-or-class-with-variable-number-of-members
#https://stackoverflow.com/questions/43069213/c-class-template-for-automatic-getter-setter-methods-good-bad-practice
#https://stackoverflow.com/questions/13980157/c-class-with-template-member-variable
#https://stackoverflow.com/questions/6261375/how-to-declare-data-members-that-are-objects-of-any-type-in-a-class
#https://stackoverflow.com/questions/1872220/is-it-possible-to-iterate-over-arguments-in-variadic-macros
#https://stackoverflow.com/questions/11761703/overloading-macro-on-number-of-arguments
#https://groups.google.com/forum/#!topic/comp.std.c/d-6Mj5Lko_s
#https://stackoverflow.com/questions/27941661/generating-one-class-member-per-variadic-template-argument
#https://stackoverflow.com/questions/14919990/c-how-to-template-a-class-attribute-and-not-the-functions-of-the-class
#https://stackoverflow.com/questions/40204338/template-parameter-pack-attribute
#https://stackoverflow.com/questions/5723619/is-it-possible-to-create-function-local-closures-pre-c11
#https://blog.feabhas.com/2014/03/demystifying-c-lambdas/
#https://stackoverflow.com/questions/1447199/c-closures-and-templates
#http://matt.might.net/articles/c++-template-meta-programming-with-lambda-calculus/
#https://www.gnu.org/software/gcc/gcc-4.4/cxx0x_status.html
#https://stackoverflow.com/questions/25091436/expand-a-parameter-pack-with-a-counter
#https://stackoverflow.com/questions/41623422/c-expand-variadic-template-arguments-into-a-statement
#https://stackoverflow.com/questions/8526598/how-does-stdforward-work
#https://stackoverflow.com/questions/7858817/unpacking-a-tuple-to-call-a-matching-function-pointer/7858971
#https://stackoverflow.com/questions/31204084/is-it-possible-to-call-static-method-form-variadic-template-type-parameter
#https://stackoverflow.com/questions/25680461/variadic-template-pack-expansion
#https://stackoverflow.com/questions/21180346/variadic-template-unpacking-arguments-to-typename
#https://stackoverflow.com/questions/2934904/order-of-evaluation-in-c-function-parameters
#https://stackoverflow.com/questions/9566187/function-parameter-evaluation-order
#https://stackoverflow.com/questions/7728478/c-template-class-function-with-arbitrary-container-type-how-to-define-it
#https://stackoverflow.com/questions/12048221/c11-variadic-template-function-parameter-pack-expansion-execution-order
#https://stackoverflow.com/questions/34957810/variadic-templates-parameter-pack-and-its-discussed-ambiguity-in-a-parameter-li
#https://stackoverflow.com/questions/20588191/error-with-variadiac-template-parameter-pack-must-be-expanded
#https://stackoverflow.com/questions/37200391/multiple-variadic-parameter-pack-for-template-class
#https://stackoverflow.com/questions/34940875/parameter-pack-must-be-at-the-end-of-the-parameter-list-when-and-why
#https://stackoverflow.com/questions/15904288/how-to-reverse-the-order-of-arguments-of-a-variadic-template-function
#https://stackoverflow.com/questions/12906523/how-can-i-interpret-a-stackdump-file
#https://stackoverflow.com/questions/320001/using-a-stackdump-from-cygwin-executable
#https://stackoverflow.com/questions/37628530/how-to-debug-using-stackdump-file-in-cygwin
#https://stackoverflow.com/questions/8305866/how-to-analyze-a-programs-core-dump-file-with-gdb
#https://stackoverflow.com/questions/5115613/core-dump-file-analysis
#https://www.linuxquestions.org/questions/programming-9/cygwin-gcc-compiled-code-generates-a-stackdump-how-to-analyze-with-gdb-874630
#http://cygwin.1069669.n5.nabble.com/exe-stackdump-and-core-dump-files-questions-td17219.html
#https://stackoverflow.com/questions/10208233/what-comes-first-template-instantiation-vs-macro-expansion/10208278#10208278
#https://stackoverflow.com/questions/44050323/class-member-variables-based-on-variadic-template
#https://stackoverflow.com/questions/40356688/c-define-a-class-member-type-according-to-its-template
#https://stackoverflow.com/questions/11394832/how-to-define-a-template-member-function-of-a-template-class
#https://stackoverflow.com/questions/29668447/c-independent-static-variable-for-each-object-inside-a-method
#http://www.cplusplus.com/forum/general/142258/
#https://stackoverflow.com/questions/3778450/is-local-static-variable-per-instance-or-per-class

!!Referências  [#Predictable_Synchronization_Algorithms_for_Asynchronous_Critical_Sections|←]
#Stefan Reif and Wolfgang Schröder-Preikschat, "Predictable Synchronisation Algorithms for Asynchronous Critical Sections,” Friedrich-Alexander-Universität Erlangen-Nürnberg, Dept. of Computer Science, Technical Reports, CS-2018-03, February 2018.
#G. Drescher and W. Schröder-Preikschat, "Guarded sections: Structuring aid for wait-free synchronisation,” in Proceedings of the 18th International Symposium On Real-Time Computing (ISORC 2015). IEEE Computer Society Press, 2015, pp. 280–283.
#S. Reif, T. Hönig, and W. Schröder-Preikschat. 2017. In the Heat of Conflict: On the Synchronisation of Critical Sections. In IEEE ISORC ’17. 42–51.
#D. Klaftenegger, K. Sagonas, and K. Winblad, "Brief announcement: Queue delegation locking,” in Proceedings of the 26th ACM Symposium on Parallelism in Algorithms and Architectures (SPAA 2014). ACM Press, 2014, pp. 70–72.
#Herlihy, Maurice. "The art of multiprocessor programming.” PODC (2006).
#Håkan Sundell, "Efficient and Practical Non-Blocking Data Structures”, Chalmers University of Technology and Göteborg University, Department of Computing Science, 2004
#Jean-Pierre Lozi , Florian David , Gaël Thomas , Julia Lawall , Gilles Muller, Remote core locking: migrating critical-section execution to improve the performance of multithreaded applications, Proceedings of the 2012 USENIX conference on Annual Technical Conference, p.6-6, June 13-15, 2012, Boston, MA
#Numa And Uma And Shared Memory Multiprocessors Computer Science Essay https://www.ukessays.com/essays/computer-science/numa-and-uma-and-shared-memory-multiprocessors-computer-science-essay.php.
#Lock-free Programming Talk at Cppcon 2014 by Herb Sutter https://www.youtube.com/watch?v=c1gO9aB9nbs&list=PLLx8RvOpJ0wlFCGxWAVBTo3CoR9PYkpz2
#An Introduction to Lock-Free Programming http://preshing.com/20120612/an-introduction-to-lock-free-programming/
#Lock-Free Programming - Geoff Langdale https://www.cs.cmu.edu/~410-s05/lectures/L31_LockFree.pdf
#P. J. Landin, "The mechanical evaluation of expressions,” The Computer Journal, vol. 6, no. 4, pp. 308–320, 1964 https://www.cs.cmu.edu/~crary/819-f09/Landin64.pdf
#An Experiment in Wait-Free Synchronisation of Priority-Controlled Simultaneous Processes: Guarded Sections, 2015 https://opus4.kobv.de/opus4-fau/frontdoor/index/index/year/2015/docId/6061
#The incremental garbage collection of processes, 1977 https://dl.acm.org/citation.cfm?id=806932
#Técnicas de Processamento Assíncrono https://ine5646.gitbook.io/livro/javascript/tecnicas-de-processamento-assincrono
#How JavaScript works: ''Event loop'' and the rise of Async programming + 5 ways to better coding with async/await https://blog.sessionstack.com/how-javascript-works-event-loop-and-the-rise-of-async-programming-5-ways-to-better-coding-with-2f077c4438b5
#C++11 Multithreading – Part 8: ''std{DIV(type="span")}:{DIV}:promise'' , ''std{DIV(type="span")}:{DIV}:promise'' and Returning values from Thread https://thispointer.com/c11-multithreading-part-8-stdfuture-stdpromise-and-returning-values-from-thread/
#CPU intensive javascript computations without blocking the single thread https://benjaminhorn.io/code/cpu-intensive-javascript-computations-without-blocking-the-single-thread/
#Promise vs Observable https://stackoverflow.com/questions/37364973/promise-vs-observable
#Promises vs Observables https://medium.com/@mpodlasin/promises-vs-observables-4c123c51fe13
#When does a thread become idle? https://stackoverflow.com/questions/19784293/when-does-a-thread-become-idle

